{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparisons\n",
    "This notebook can be used to compare models agaist each other.\n",
    "\n",
    "## Preperation\n",
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gif\n",
    "from tf.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, precision_recall_curve, \\\n",
    "    recall_score, roc_auc_score, roc_curve, plot_roc_curve\n",
    "from math import pi\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed, Layout\n",
    "import preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [...]\n",
    "\n",
    "model_names = []\n",
    "for path in model_paths:\n",
    "    model_names.append(path.split('/')[1].strip('.h5'))\n",
    "\n",
    "models = []\n",
    "for model in model_paths:\n",
    "    models.append(load_model(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = preprocess.labeled_data(\n",
    "    'images/multi_category/test/images/', \n",
    "    'images/multi_category/test/labels/', \n",
    "    resize=(480, 480))\n",
    "\n",
    "print(f'X_test shape = {X_test.shape}\\ny_test shape = {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('background', 'class1', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Predictions with the models__\n",
    "* Store list of categorical predictions in `y_preds`\n",
    "* Combine prediction channels into a single channel `y_preds_combined`.  Note that this is a simple combination method that assigns the class of the pixel to the channel with the highest probability prediction.  More acurate models may be possible if they take different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "for model in models:\n",
    "    y_preds.append(model.predict(X_test))\n",
    "\n",
    "\n",
    "def combine(array):\n",
    "    rtn_array = np.zeros_like(array[:, :, :, 0])\n",
    "    for batch, _ in enumerate(array):\n",
    "        for row, _ in enumerate(array[0]):\n",
    "            for column, _ in enumerate(array[0, 0]):\n",
    "                rtn_array[batch, row, column] = array[batch, row, column, :].argmax()\n",
    "    return rtn_array\n",
    "\n",
    "\n",
    "def combine_advanced(array, channel, thresh):\n",
    "    rtn_array = np.zeros_like(array[:, :, :, 0])\n",
    "    for batch, _ in enumerate(array):\n",
    "        for row, _ in enumerate(array[0]):\n",
    "            for column, _ in enumerate(array[0, 0]):\n",
    "                if array[batch, row, column, channel] > thresh:\n",
    "                    rtn_array[batch, row, column] = channel\n",
    "                else:\n",
    "                    rtn_array[batch, row, column] = array[batch, row, column, :].argmax()\n",
    "    return rtn_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual\n",
    "### All classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_comparison_combined(models, X_test, y_test, y_pred, test_image=0, xzoom=None, yzoom=None):\n",
    "    rows = int( (len(models) / 2) + 1.5 )\n",
    "    plt.figure(figsize=(15, rows * 8))\n",
    "\n",
    "    plt.subplot(rows, 2, 1)\n",
    "    plt.imshow(X_test[test_image])\n",
    "    plt.title('original')\n",
    "    plt.xlim(xzoom); plt.ylim(yzoom)\n",
    "\n",
    "    plt.subplot(rows, 2, 2)\n",
    "    plt.imshow(combine(y_test)[test_image])\n",
    "    plt.title('ground truth')\n",
    "    plt.xlim(xzoom); plt.ylim(yzoom)\n",
    "\n",
    "    for i, y_pred in enumerate(y_preds):\n",
    "        plt.subplot(rows, 2, 3+i)\n",
    "        plt.imshow(combine(y_pred)[test_image])\n",
    "        plt.title(model_names[i] + '    prediction')\n",
    "        plt.xlim(xzoom); plt.ylim(yzoom)\n",
    "        \n",
    "\n",
    "visual_comparison_combined(models, X_test, y_test, y_pred, 0, (200, 400), (300, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defect class only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_comparison_channel(defect_class, test_image, classes, models, X_test, y_test, y_pred, model_names,\n",
    "                              xzoom=None, yzoom=None):\n",
    "    print(f'Probability predictions for {classes[defect_class]} class')\n",
    "\n",
    "    rows = int( (len(models) / 2) + 1.5 )\n",
    "    plt.figure(figsize=(15, rows * 8))\n",
    "\n",
    "    plt.subplot(rows, 2, 1)\n",
    "    plt.imshow(X_test[test_image])\n",
    "    plt.title('original')\n",
    "    plt.xlim(xzoom); plt.ylim(yzoom)\n",
    "\n",
    "    plt.subplot(rows, 2, 2)\n",
    "    plt.imshow(y_test[test_image, :, :, defect_class])\n",
    "    plt.title('ground truth')\n",
    "    plt.xlim(xzoom); plt.ylim(yzoom)\n",
    "\n",
    "    for i, y_pred in enumerate(y_preds):\n",
    "        plt.subplot(rows, 2, 3+i)\n",
    "        plt.imshow(y_pred[test_image, :, :, defect_class])\n",
    "        plt.title(model_names[i] + '    prediction')\n",
    "        plt.xlim(xzoom); plt.ylim(yzoom)\n",
    "        plt.colorbar()\n",
    "\n",
    "\n",
    "args = classes, models, X_test, y_test, y_pred, model_names\n",
    "visual_comparison_channel(2, 0, *args, (180, 400), (320, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Accuracy, [precission and recall](https://en.wikipedia.org/wiki/Precision_and_recall) for predictions from seperate channels.\n",
    "\n",
    "Calculate scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(defect_class, threshold, y_test, y_pred):\n",
    "    accuracies, precisions, recalls = [], [], []\n",
    "    for y_pred in y_preds:\n",
    "        y_score, y_pred = y_test[:, :, :, defect_class].ravel(), y_pred[:, :, :, defect_class].ravel() > threshold\n",
    "        accuracies.append( accuracy_score(y_score, y_pred) )\n",
    "        precisions.append( precision_score(y_score, y_pred) )\n",
    "        recalls.append( recall_score(y_score, y_pred) )\n",
    "    return accuracies, precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_bar_charts(accuracies, precisions, recalls, model_names, defect_class=None, threshold=None, figsize=(17, 4)):\n",
    "    acc = np.array(accuracies)\n",
    "    acc = (acc - acc.min()) / (acc.max() - acc.min())\n",
    "    if defect_class and threshold:\n",
    "        print(f'scores for {classes[defect_class]}, threshold = {threshold}')\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, (metric, title) in enumerate(zip([accuracies, precisions, recalls], ['Accuracy', 'Precision', 'Recall'])):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.bar(model_names, metric, color=plt.rcParams['axes.prop_cycle'].by_key()['color'], alpha=0.9)\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=90)\n",
    "        if i == 0:\n",
    "            low = np.array(accuracies).min()\n",
    "            plt.ylim(low * (1 - ((1 - low) * 0.5)) , 1)\n",
    "\n",
    "        \n",
    "accuracies, precisions, recalls = scores(2, 0.1, y_test, y_pred)\n",
    "scores_bar_charts(accuracies, precisions, recalls, model_names, 2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_scores(defect_class, threshold, y_test, y_pred, model_names, colors=None):\n",
    "    accuracies, precisions, recalls = scores(defect_class, threshold, y_test, y_pred)\n",
    "    labels = ['Accuracy', 'Precision', 'Recall']\n",
    "    if not colors:\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    for acc, presn, rec, color, label in zip(accuracies, precisions, recalls, colors, model_names):\n",
    "        values = [acc, presn, rec, acc]\n",
    "        ax.plot(angles, values, label=label, color=color)\n",
    "        ax.fill(angles, values, color=color, alpha=0.05)\n",
    "\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_thetagrids(np.degrees(angles), labels)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_rlabel_position(180)\n",
    "    for label, angle in zip(ax.get_xticklabels(), angles):\n",
    "        if angle in (0, np.pi):\n",
    "            label.set_horizontalalignment('center')\n",
    "        elif 0 < angle < np.pi:\n",
    "            label.set_horizontalalignment('left')\n",
    "        else:\n",
    "            label.set_horizontalalignment('right')\n",
    "    ax.set_title('Model metrics', y=1.08)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "    ax.text(5.5, 1.5, f'threshold = {threshold:.2f}', size=15)\n",
    "\n",
    "    \n",
    "def widget_radar_scores(defect_class, threshold):\n",
    "    defect_class = classes.index(defect_class)\n",
    "    radar_scores(defect_class, threshold, y_test, y_pred, model_names, ['blue', 'purple', 'red', 'orange'])\n",
    "\n",
    "\n",
    "interact(widget_radar_scores,\n",
    "         defect_class=widgets.Dropdown(options=classes, value='tape', desription='class:'),\n",
    "         threshold=widgets.FloatSlider(min=0, max=1, step=0.05, value=0.5, layout=Layout(width='70%')));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gif of radar charts accross thresholds can be created with the cell below *(note; can take a long time if the number of frames is high)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "@gif.frame\n",
    "def plot(threshold):\n",
    "    radar_scores(2, threshold, y_test, y_pred, model_names, ['blue', 'purple', 'red', 'orange'])\n",
    "\n",
    "frames = []\n",
    "for threshold in np.linspace(0.01, 0.99, 20):\n",
    "    frames.append( plot(threshold) )\n",
    "\n",
    "gif.save(frames, 'test.gif', duration=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_curve(defect_class, y_test, y_preds, model_names, classes):\n",
    "    aps = []\n",
    "    y_test = y_test[:, :, :, defect_class].ravel()\n",
    "    for model, y_pred in enumerate(y_preds):\n",
    "        y_score = y_pred[:, :, :, defect_class].ravel()\n",
    "        ap = average_precision_score(y_test, y_score)\n",
    "        aps.append(ap)\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n",
    "        plt.plot(recall, precision, label=f'{model_names[model]}  AP = {ap:.3f}')\n",
    "    plt.plot([0, 1], [1, 0], ls='--', c='grey', zorder=0, alpha=0.3)\n",
    "    plt.title(classes[defect_class])\n",
    "    plt.xlabel('recall'); plt.ylabel('precision')\n",
    "    plt.axis('equal')\n",
    "    plt.grid()\n",
    "    plt.legend(loc='lower left')\n",
    "    \n",
    "\n",
    "def pr_curves(classes, y_test, y_preds, model_names):\n",
    "    rows = int(len(classes) / 2 + 0.5)\n",
    "    plt.figure(figsize=(15, rows*6 + 1))\n",
    "    plt.suptitle('Precision-Recall curves for each class (considered seperately)', y=1.02, size=15)\n",
    "    for channel, cls in enumerate(classes):\n",
    "        plt.subplot(rows, 2, channel+1)\n",
    "        pr_curve(channel, y_test, y_preds, model_names, classes)\n",
    "    plt.tight_layout()\n",
    "            \n",
    "        \n",
    "pr_curves(classes, y_test, y_preds, model_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
